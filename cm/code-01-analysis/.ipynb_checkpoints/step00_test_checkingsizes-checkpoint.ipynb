{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import sys\n",
    "sys.path.append('/Users/cmagri1/OneDrive - Johns Hopkins/git')\n",
    "from torch import nn\n",
    "import torch\n",
    "from torchvision.models.alexnet import alexnet\n",
    "from tqdm import tqdm\n",
    "\n",
    "from EncodingModel_cm296 import utils as emutils\n",
    "from EncodingModel_cm296 import feature_extractor as emfe\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load word2sense\n",
    "#already preprocessed in MATLAB so that Wrd2Sns and THINGs overlap --> we have IMAGES, LABELS and SENSES\n",
    "pathtofile = '../code-00-preprocessdataset/'\n",
    "PathToImgs = 'img-test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute DNN features\n",
    "def object_by_feature(PathToImgs,keyword,ilayer,resolutionval,WordInf=[],pretrained=True):\n",
    "\n",
    "    print('Computing '+ keyword+ ' stimuli features for ' + ilayer)\n",
    "    #Specify the model\n",
    "    model = emfe.AlexNet(ilayer,pretrained_val=pretrained);      \n",
    "    condition_features = emutils.compute_features(model, PathToImgs,resolutionval,WordInf)\n",
    "    return condition_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing test stimuli features for conv_5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 11.93it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "keyword = 'test'\n",
    "ilayer = 'conv_5'\n",
    "resolutionval= 224\n",
    "object2vec_features = object_by_feature(PathToImgs,keyword,ilayer,resolutionval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9216,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object2vec_features['acorn'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torchvision.models.alexnet import alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base = alexnet(pretrained=True)\n",
    "base.features[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_5 = base.features[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (4): ReLU(inplace=True)\n",
       "  (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (7): ReLU(inplace=True)\n",
       "  (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (9): ReLU(inplace=True)\n",
       "  (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base = alexnet(pretrained=True)\n",
    "base.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNetConv5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        base = alexnet(pretrained=True)\n",
    "        self.conv = base.features\n",
    "        self.conv1 = base.features[:2]\n",
    "        self.avgpool = base.avgpool\n",
    "        \n",
    "        self.eval()\n",
    "\n",
    "    def forward(self, stimuli):\n",
    "        x = self.conv(stimuli) #shape: [stim, 256, 7, 7] / mean test set r = 0.7297\n",
    "        #x = self.avgpool(x) #shape: [stim, 256, 6, 6]\n",
    "        x = x.view(x.size(0), -1) #shape: [stim, 12544] / mean test set r =  0.7218\n",
    "        #x = self.fc_6(x) #shape: [stim, 4096] / mean test set r = 0.7142\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load word2sense\n",
    "#already preprocessed in MATLAB so that Wrd2Sns and THINGs overlap --> we have IMAGES, LABELS and SENSES\n",
    "savepath = '../../../data-00/'\n",
    "\n",
    "#Parameters\n",
    "pretrainedModel = False\n",
    "resolutionval = 227;\n",
    "\n",
    "\n",
    "layer = 'conv_5';\n",
    "# ROI = {'EVC','LOC'}\n",
    "ROI = 'EVC'\n",
    "Sub = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.spatial as sp\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.load(savepath + 'Weights_Sub' + str(Sub) + '_' + ROI + \"_\" + layer + '.npy')\n",
    "w_r = np.load(savepath + '/Weights_Sub' + str(Sub) + '_' + ROI + \"_\" + layer + '_random.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "results2 = 1 - sp.distance.cdist(w.transpose()[:120,:], w_r.transpose()[:120,:], 'correlation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7facb5dddd50>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(results2, center=0)\n",
    "# sns.heatmap(results2, center=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[        nan,         nan,         nan],\n",
       "       [        nan,         nan,         nan],\n",
       "       [-0.07207374,  0.07587055, -0.04143192]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
